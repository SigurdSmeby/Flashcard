type,topic,question,answer,options,correctOption,explanation
mc,Lecture 2,"Hva er forskjellen mellom et problem og en problem-instans innenfor algoritmer?",,"Et problem er vanskeligere √• l√∏se enn en problem-instans|Et problem er en generell beskrivelse, mens en problem-instans er et spesifikt tilfelle med konkrete inndata|Et problem krever alltid en algoritme, mens en problem-instans kan l√∏ses manuelt|Et problem har kun √©n l√∏sning, mens en problem-instans kan ha flere l√∏sninger",1,"Et problem er en generell beskrivelse av hva inndataene ser ut som og hva den √∏nskede utdataen skal v√¶re, for eksempel sortering av en liste. En problem-instans er et spesifikt tilfelle av problemet med konkrete inndata, som for eksempel listen [3, 9, 1, 42, 17]."
mc,Lecture 2,"Forklar hva asymptotisk notasjon brukes til i forbindelse med algoritmeanalyse.",,"For √• m√•le n√∏yaktig kj√∏retid i sekunder for alle typer algoritmer|For √• analysere hvordan kj√∏retiden oppf√∏rer seg|For √• telle antall linjer kode i en algoritme|For √• sammenligne konstante faktorer mellom algoritmer",1,"Asymptotisk notasjon er et verkt√∏y for √• analysere hvordan kj√∏retiden til en algoritme (eller veksten til en matematisk funksjon) oppf√∏rer seg n√•r st√∏rrelsen p√• inndataene blir veldig stor. Det hjelper oss med √• sammenligne effektiviteten til forskjellige algoritmer ved √• fokusere p√• den dominerende veksttermen og ignorere konstante faktorer og lavere ordens termer."
qa,Lecture 2,"Nevn og beskriv kort de tre hovedkategoriene av asymptotisk notasjon som ble presentert.","Big-O notasjon (ùëÇ) gir en √∏vre grense for veksten til en funksjon; ùëì(ùëõ) = ùëÇ(ùëî(ùëõ)) betyr at ùëì ikke vokser raskere enn ùëî. Big-omega notasjon (Œ©) gir en nedre grense; ùëì(ùëõ) = Œ©(ùëî(ùëõ)) betyr at ùëì ikke vokser saktere enn ùëî. Big-theta notasjon (Œò) betyr at to funksjoner vokser i samme takt; ùëì(ùëõ) = Œò(ùëî(ùëõ)) betyr at ùëì og ùëî har samme vekstrate.",,,
qa,Lecture 2,"Hva er den typiske strukturen som kjennetegner sorteringsalgoritmer med kvadratisk tidskompleksitet?","Kvadratiske sorteringsalgoritmer har typisk to n√∏stede l√∏kker der begge l√∏kkene itererer over (omtrent) alle elementene i inndataene. Dette resulterer i et antall operasjoner som er proporsjonalt med kvadratet av inndatast√∏rrelsen (ùëõ¬≤).",,,
qa,Lecture 2,"Hvorfor har sammenligningsbaserte sorteringsalgoritmer en nedre grense p√• Œ©(ùëõ log ùëõ) i verste fall?","Sammenligningsbaserte sorteringsalgoritmer bestemmer rekkef√∏lgen av elementer kun ved √• sammenligne par av elementer. Det er bevist at i verste fall kreves det minst logaritmen av antall mulige permutasjoner (som er ùëõ!) for √• garantere at listen er sortert, og log‚ÇÇ(ùëõ!) er i Œ©(ùëõ log ùëõ).",,,
qa,Lecture 2,"Gi et eksempel p√• en operasjon som ikke regnes som et primitivt steg i kompleksitetsanalyse og forklar hvorfor.","Sammenligning eller konkatenering av strenger regnes vanligvis ikke som et primitivt steg. Dette er fordi operasjonene kan ta en tid som er proporsjonal med lengden p√• strengene, og dermed ikke en konstant tidsbruk uavhengig av inndatast√∏rrelsen.",,,
qa,Lecture 2,"Hva er et viktig hensyn √• ta n√•r man analyserer tidskompleksiteten til kode som bruker bibliotekfunksjoner?","N√•r man analyserer kode med bibliotekfunksjoner, er det viktig √• kjenne til (eller unders√∏ke) tidskompleksiteten til disse funksjonene. En enkelt linje kode som kaller en bibliotekfunksjon kan skjule en ikke-konstant tidsoperasjon, noe som p√•virker den totale kompleksiteten til algoritmen.",,,
qa,Lecture 2,"Beskriv ""bad painter's algorithm"" i forbindelse med strengkonkatenering i spr√•k med immutable strings.","""Bad painter's algorithm"" refererer til ineffektiv gjentatt konkatenering av strenger i spr√•k der strenger er immutable. Hver konkatenering skaper en ny streng og kopierer innholdet fra de gamle strengene, noe som f√∏rer til en kvadratisk tidskompleksitet (Œò(ùëõ¬≤)) hvis man gjentatte ganger legger til sm√• strenger til en stadig voksende streng.",,,
qa,Lecture 2,"Hva er den grunnleggende ideen bak amortisert tidskompleksitet, og i hvilke situasjoner kan det v√¶re en ulempe?","Amortisert tidskompleksitet beskriver den gjennomsnittlige kostnaden per operasjon over en sekvens av operasjoner, selv om enkelte individuelle operasjoner kan ha en h√∏yere kostnad. Dette oppst√•r ofte n√•r man dynamisk endrer st√∏rrelsen p√• datastrukturer, som for eksempel ved √• doble st√∏rrelsen p√• en array n√•r den er full. I sanntidssystemer med strenge tidsbegrensninger kan den variable kj√∏retiden til individuelle operasjoner v√¶re problematisk.",,,
qa,Lecture 2,"Nevn to eksempler p√• velkjente algoritmer eller datastrukturer (uten √• beskrive hvordan de fungerer) og forklar kort hvilke problemer de l√∏ser.","Et balansert bin√¶rt s√∏ketre er en datastruktur som tillater effektiv s√∏king, innsetting og sletting av elementer i logaritmisk tid. Dijkstras algoritme er en algoritme for √• finne den korteste stien fra en gitt startnode til alle andre noder i en vektet graf uten negative kantvekter.",,,
qa,Lecture 3,"Hva er en hashmap, og hvordan oppn√•r den ""nesten konstant"" tidskompleksitet for mange operasjoner?","En hashmap er en datastruktur som lagrer n√∏kler og tilh√∏rende verdier. Den bruker en hashfunksjon for √• beregne en indeks i et internt array basert p√• n√∏kkelen, noe som gir rask tilgang, innsetting og sletting i forventet konstant tid, selv om kollisjoner kan p√•virke ytelsen i verste fall.",,,
qa,Lecture 3,"Hvorfor er det viktig √• implementere __hash__ og __eq__ (i Python) eller hashCode() og equals() (i Java) n√•r man bruker egendefinerte k","Disse metodene er essensielle for at hashmaps skal fungere korrekt med egendefinerte n√∏kler. __hash__/hashCode() bestemmer hvor objektet skal plasseres, mens __eq__/equals() brukes til √• sjekke om to n√∏kler er like, spesielt ved kollisjoner. Konsistens mellom disse to er avgj√∏rende: like objekter m√• ha samme hashkode",,,
qa,Lecture 3,"Beskriv hvordan dybde-f√∏rst s√∏k (DFS) fungerer for √• traversere en graf.","DFS fungerer ved √• starte ved en node, bes√∏ke en av dens ubes√∏kte naboer, og deretter rekursivt fortsette fra den naboen. Den utforsker en ""sti"" s√• dypt som mulig f√∏r den backtracker til forrige node og utforsker en annen sti. For √• unng√• uendelige l√∏kker i grafer med sykluser, m√• algoritmen markere bes√∏kte noder.",,,
qa,Lecture 3,"Hva er hovedproblemet med en rekursiv implementasjon av DFS for sv√¶rt store grafer eller tr√¶r, og hvordan kan man omg√• dette problemet?","En rekursiv DFS-implementasjon kan f√∏re til stack overflow errors for store grafer eller tr√¶r fordi hvert rekursive kall legger til et nytt lag i call stacken, som har begrenset st√∏rrelse. Dette kan unng√•s ved √• implementere DFS iterativt ved hjelp av en eksplisitt stack data struktur (som en liste eller en dedikert stack-klasse) som administreres p√• heapen, som har mye st√∏rre kapasitet.",,,
qa,Lecture 3,"Forklar konseptet memoization og gi et eksempel p√• n√•r det er spesielt nyttig.","Memoization er en optimaliseringsteknikk for rekursive funksjoner der resultatet av kostbare funksjonskall lagres og gjenbrukes for samme input. Det er nyttig for problemer med overlappende subproblemer, for eksempel beregning av Fibonacci-tall rekursivt, der mange delberegninger gj√∏res flere ganger.",,,
qa,Lecture 3,"Gi et kort eksempel p√• en listekomprenasjon i Python og forklar dens tidskompleksitet i forhold til antall elementer den opererer p√•. Eksempel: [x*2 for x in numbers if x > 0].","Denne listekomprenasjonen g√•r gjennom alle elementene i numbers. Hvis betingelsen x > 0 er sann, utf√∏res transformasjonen x*2 og resultatet legges til i den nye listen. Hvis det er n elementer i numbers, og betingelsen og transformasjonen tar konstant tid, vil tidskompleksiteten v√¶re Œò(n).",,,
qa,Lecture 3,"Hva er dynamisk programmering (DP), og hvordan skiller det seg fra memoization i implementeringen?","Dynamisk programmering er en teknikk for √• l√∏se komplekse problemer ved √• bryte dem ned i overlappende subproblemer, l√∏se hvert subproblem bare √©n gang og lagre l√∏sningene i en tabell. Mens memoization typisk er en top-down tiln√¶rming (rekursivt med lagring), er DP ofte en bottom-up tiln√¶rming (iterativt, starter med de enkleste subproblemene).",,,
qa,Lecture 3,"Hvilken viktig egenskap kjennetegner NP-komplette problemer med hensyn til verifisering av en foresl√•tt l√∏sning?","En viktig egenskap ved NP-komplette problemer er at selv om det kan v√¶re vanskelig √• finne en l√∏sning, kan korrektheten av en gitt ""ja""-l√∏sning verifiseres i polynomisk tid. Dette betyr at hvis noen presenterer en mulig l√∏sning, kan vi relativt raskt sjekke om den faktisk er gyldig.",,,
qa,Lecture 3,"Nevn to eksempler p√• NP-komplette beslutningsproblemer som ble nevnt i kildematerialet.","To eksempler p√• NP-komplette beslutningsproblemer er Boolean satisfiability (SAT) og Subset sum. Andre inkluderer Subgraph isomorphism og Traveling salesperson.",,,
qa,Lecture 3,"Hva er den viktigste fordelen med generatorkomprehensjoner i Python og stream expressions i Java sammenlignet med listekomprenhensjoner n√•r det gjelder minnebruk og ytelse i visse situasjoner?","Generatorkomprehensjoner og stream expressions er ""lazy evaluated"", noe som betyr at de genererer elementer kun n√•r de ettersp√∏rres, i stedet for √• lage hele datastrukturen p√• forh√•nd. Dette kan v√¶re mer minneeffektivt, spesielt for store datasett, og kan forbedre ytelsen hvis ikke alle genererte elementene faktisk brukes.",,,
